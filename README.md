# tower-of-hanoi-RL
Abstract

Introduction
“How does the mind work, or at least, how does part of it seem to work?” is perhaps some of the most intriguing questions hunting human curiosity. Machine learning is such a field that has ventured itself in this even-inquiring topic. As this field of study progressed and matured throughout the decades, with its golden ages and winter periods, it has shaded light on possible ways how the part of the brain functions on certain tasks like image recognition and text translation, far surpassing human capabilities. Such findings have gotten us nearer, or perhaps more likely helped us realize how far are we from solving the dichotomy of shallow and general intelligence. In this study, I discuss some of the findings that AI has taught me as a result of reflections and insights during my exploration in learning methods like deep learning and reinforcement learning, sharing my interpretation of general intelligence and how to achieve it, which also means pointing what are the current limitations. 
Perhaps an interesting starting point is sharing from where can we derive the confidence that achieving general intelligence is a possible, reachable goal instead of just some illusionary expectation. The most convincing argument for me has been the astronomical difference in the times of development between the human brain, i.e. natural intelligence and the machine “brain”, i.e. artificial intelligence. The former is the result of tens of thousands of years of evolution. This is about 70000 years ago since Homo sapiens as a species underwent a so-called Cognitive Revolution that seemingly ignited the development of “cultures”, which contribute to the cognitive and behavioral basis for today’s human beings  (Harari, 2014). By a stark contrast, artificial intelligence began as a systematic field of study not even a century ago, in 1956 in an academic conference led by McCarthy et al as part of the Dartmouth summer research project (McCarthy, et al., August 31, 1955). Although it’s also true that it’s difficult to precisely trace the time when humans began hypothesizing and asking about how machines could perform, and outperform, humans at human tasks, with a notable example of this in 1950 (about 6 years before the Dartmouth conference) when Alan Turing published the renowned paper Computing Machinery and Intelligence, where he proposed the notion of machines being able to simulate human thinking and perform intelligent actions, as well as the Turing test to test such machine (Smith, et al., 2006). Regardless of the imprecise date when the field of machine intelligence was birthed, it shouldn’t undermine the breakthroughs achieved by AI, with models like GPT-3 for language production, AlphaFold for mapping gene sequences to protein structures, thus simplifying human scientific work that would otherwise take about a decade, AlphaGo as a model that defeated humans at the game of Go, among others . Although these are all achievements falling into the field of shallow AI, because of the small frame of time it took to get there, it does provide us the provocative thought that it’s just an event asymptotically bound to happen that AI will continue to be able, on one hand, to further approximate its functions to that of human faculties, and on the other, continue to polish skill where it already far surpassed humans, such as by decreasing the consumption of computational resources while still retaining the hight capabilities. From another perspective, wouldn’t it be unthinkable, and at the same time unprecedently strange, that Homo sapiens 70000 years ago could develop some “breakthough” capacities  in the field of hunting, fire, language communication in a matter of 7 decades？By this comparison, achievements in shallow AI in just a few decades are indeed getting us nearer to general AI, or even better, helped us substantially to realize how farther away are we from unraveling the mysteries of tens of thousands of years of evolution.	 
On a practical approach to approximate the current state of AI to general intelligence
There have been a plethora of machine learning algorithms developed during this relatively short time. The few that have left a footprint in my mind are deep learning and reinforcement learning, as they serve as foundations for understanding human cognition, and due to their influence spread through the AI community. 
At their core, deep learning seems to be focused on reducing prediction error in a supervised setting (please see (Goodfellow, et al., 2016)), while reinforcement learning focuses on maximizing a reward gained from a state, a sample from reality, that is accessible through an action from an action space (please see (Sutton & Barto, 2018, 2020)). 
Indeed, they are fundamentally different and solve different kinds of tasks, for instance, deep learning can be used for image recognition as ground-truth images can be fed to a network in order to finely tune its weights, while reinforcement learning hasn’t a ground truth, per se, but rather seeks to maximize a reward by observing what actions in a state are expected to do so. But what I prefer to see is that they’re complementary to each one. 
The idea behind the design of deep learning algorithms seem a viable practical implementation for recent interpretations of how the brain works, mainly predictive processing. (Clark, 2013) has shared that brains, perhaps, are essentially prediction machines structured as a hierarchical generative model aimed at minimizing prediction error, which is a viable view to consider given that it serves to unify other faculties such as perception, action and attention under a common framework. Id Est, a framework made of probability-density distributions induced by hierarchical generative models as our basic means of representing the world, and prediction-error minimization as the driving force behind learning, action-selection, recognition, and inference.
Reinforcement learning, on the other hand, seems to be complemented by (Zhu, 2017). In his book, Zhu emphasizes the need to seek a fundamental theory that unifies the different findings from the diverse fields of AI, ranging from causal reasoning, cognition, language production, perception-action mapping, among others. This is done while stressing how current shallow AI’s capabilities pale in comparison to a crow, an animal which can understand the environment it’s put in, how it should interact with it in order to crack a walnut and get the food inside it. Finding a way to represent the environment, and understanding how actions influence it doesn’t seem to be tasks that can be solved through deep learning, e.g., there is no such a thing as a known ground truth, thus, no supervision. Reinforcement learning acts as a way to complement it given that it involves providing the agent with a value function that varies depending on the actions an agent executes in a given state. It’s worth noting that Zhu does point out that studies based on RL limit themselves in simple symbolic space, as well as when robots are taught how to perform an action, such as grabbing a bottle in a fixed position, the action is “brittle”, as the robot fail to do so once the object’s position changes (Zhu, 2017). 
A particular issue I identify in shallow AI is the difficulty to diversify tasks for a single model. Although breakthroughs such as AlphaGo, AlphaFold, and GPT-3 excel at their respective tasks of playing Go, mapping gene sequences to proteins and producing language, they beget the question of whether there’s anything else they can do. Could AlphaGo predict the music preferences of the user, AlphaFold be able to design a prevention strategy based on the same gene sequence input or GPT-3 recognize images? 

Given the above premise, it’s hypothesized that perhaps there can be an algorithm that integrates the core functionalities of both types of learning. This algorithm might be designed to embed the agent with the following faculties:
Goal	Mathematical implementation
Develop an inner representation of the environment	Take a sample from an infinite state space that’s perceived as an input from the environment. The sample is taken through actions, like reinforcement learning. Unlike gaming, the sample’s features, such as patterns, can be learned with deep learning. 
Reduce prediction error and map it into action	Use techniques of gradient descent from DL to reduce the prediction error and map it to actions to accomplish a task and interact with environment
Seek a motivation, which allows the diversification of tasks	Get tasks trained from DL, and allow the agent to carry out these tasks through RL techniques. This would cover up for the pitfalls of shallow AI, which is task centered, thus is only capable of exceling at one single task like text production. 
	
	
Table 1: Desired traits to embed on a well-known agent and their possible mathematical realizations. 
Motivation centered instead of task centered: a well-known agent before an all-known agent
Even though I’ve mentioned that achieving general AI is asymptotically bound to happen, I believe it’s still a matter of process. By analogy of the history of AI, intelligent agents were not built at the beginning through neural networks that extracted rules from a supervised setting, but rather from a large amount of heuristic logic expressions meant to resemble reasoning. This of course found its pitfalls, for instance, one can wonder: how many rules would one need to produce a felicitous sentence in English? However, it’s perhaps this obstacle that spurred the curiosity and creativity that led to neural networks that don’t even rules, per se, but rather fed a large amount of felicitous English sentences so that a model tweak its own weights and extract the rules from it. 
The same process of trial and error can be mapped to the quest of searching general AI: it may be that we may have to reach an “intermediate” AI, instead of directly leaping from shallow to general AI. My proposal is that this intermediate AI can evolve from shallow AI in the following way: it’s domain-centered instead of task-centered. As pointed earlier, being task-centered lead to an agent that can readily outperform humans, but only in a particular task. If it’s domain centered, however, then an agent is specialized in perform a range of diverse tasks under a common theme. For example, a domain-centered agent specialized in the field of medicine would be able to do the following by merging together the different models presented earlier:
-	GPT-3: transfer learning to be able to write prescriptions given an input a conversation between patient and doctor (natural language processing)
-	Personalized recommendation of activities to do given an input the constant monitoring of health parameters like heart rate and biomarkers (machine learning)
-	Ability to recognize diseases like malignant tumors from CT, X-ray scans (image recognition) 
-	Learn how to perform surgeries, organize a medical environment (machine learning) 
-	Among others. (Eric, 2015) (Eric, 2019)
A way to achieve this might be summing up these different models and use transfer learning to tweak weights and biases under the above common theme.  Of course, human intervention at these key stages of development should be automatized in order to achieve general AI, nonetheless, a multi-purpose, adaptive, “jack of all trades” agent able to perform diverse tasks under the same domain is theoretically better than an agent which is the master of one skill.  
Another evident limitation of such domain-centered agent is that it wouldn’t be able to excel, nor even perform, tasks outside this domain of medicine. For instance, it wouldn’t be able to outperform humans at games like Go or chess (although it could be argued that it’s good for the mental health of the patient to have an intelligent agent learn to play such games). 
The anthropomorphic misled: why think that AI will dominate humans and not borrow resources, why fall into the fear
Of course, although I envision what are the future steps that can be taken to get nearer towards general AI, there are possible worries whether super-intelligent would pose an overall threat to humankind, such as acquiring of all resources available to increase computational power even if it’s at the cost of humankind’s existence (Bostrom, 2015), and if yes, why bother in embarking a possibly suicidal-path. 
Throughout my studies of machine learning, I’ve constantly come across the personification of hardware and software, which causes a phenomenon I’d like to call the “anthropomorphic misled”.  
To understand this term, I’ll first employ an example with nature and then make an analogy to AI. It’s well known that water undergoes a cycle whereby evaporation of it leads to condensation in the atmosphere, and sufficient condensation then leads to rain. The anthropomorphic misled in this situation would be to interpret such natural phenomenon as “water” has resuscitated itself, with resuscitation an idea that humans came up with, yet embed on living and non-living things around it. Something similar tends to happen in machine learning, whereby the phenomenon of anthropomorphic misled arises when humans embed features on models, which behaviors arise from mathematical phenomena. For example, although we claim that a model fed with millions of images can “extract generic features from them”, and then “learn” to recognize them, to avoid the anthropomorphic misled where we embed these human faculties of generalizing, learning and recognition, I’ll very much prefer and recommend what the underlying algorithms are doing and how is the neural architecture changing. Error reduction is a cover up term for weights’ adjustments in a matrix, which are in turn cover up for taking the derivatives of error functions which decrease the error function until a local or global minimum is achieved. The model isn’t “trained” per se, but ran over a large number of iterations to get that error function low. Whenever the model is ran on an input and outputs a correct label to it, we could say that such input led to some positive output for some rectified linear unit function, zero otherwise, instead of falling into the anthropomorphic misled of saying the model “recognized” features of an image, hence gave it the correct label. Table 1 is also yet another example of how mathematical formalizations are fundamentally different to what humans refer as “understanding”, “learning”,.

 
(Zeiler & Fergus, 2013)
The inherent vs. inherit dichotomy for fearing intelligent agents
Under normal circumstances, that is, where I learned about different kinds of machine learning methods, I wasn’t concerned about falling into the anthropomorphic misled. I liked to use terms such as the model has learned to recognize cats, the agent is seeking rewards, the agent has defeated the world champion in Go, among others. Shedding light into the intricacies of the mind has indeed been an interesting process. Influeced by deep neural networks, I’d sometimes think that maybe that’s part of how part of my brain works. For instance, whenever this multiplication comes: 86 * 25, it may be that certain neurons may be firing as their weights were adjusted in childhood by multiplication tables that served as training data.  
However, the fear that has spurred from AI, such as those concerning invading and exterminating humans in a competition for computational power, or racial biases leading to unjust arrests, are also perhaps the result of the anthropomorphic misled, and if they aren’t addressed properly, I fear it may stall the development of efficient AI that can assist and benefit humanity. Particular worries, such as those laid out in (Bostrom, 2015), may arise because we human tend to imprint our traits on AI agents. There exists neutral traits, that is, those that can’t qualify as neither good nor bad, such as a model “has learnt…”. Nonetheless, we could also imprint those negative human traits that exacerbate humans’ fears, such as “agents might exterminate/conquer/enslave humankind”, foreshadowing perhaps how we might be afraid of our own shadows (Harari, 2014). Indeed, in Sapiens: A Brief History Of Humankind, I was impacted by how mankind’s history, or speaking in a more biologically precise way, Homo sapiens history has indeed been that full of unnecessary bloodshed, conquest and enslavement between each other instead of peaceful cooperation, of blind extermination of other species and nature instead of symbiotic coexistence, so it’s, to some extent, reasonable to fall into the anthropomorphic misled of assuming general AI, or intelligent agents may treat us in a similar way we’ve historically treated one another. Indeed, if intelligent systems do inherit Homo sapiens’s intolerance (mathematically speaking this might be materialized as weights and biases  of an omnipotent model), then general AI will indeed be a threat to mankind. However, this threat is not inherent of the system perhaps; as remarked, this is the case if they inherit our intolerance. 
Said otherwise, intelligent agents are not to be feared, and the path towards general AI shouldn’t be hindered by fear of our own shadows in history driven by the anthropomorphic misled. 
